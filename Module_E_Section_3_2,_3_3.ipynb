{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM5Sli0dyOwpsVWuXAm6Sl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayalazech/MAT-421/blob/main/Module_E_Section_3_2%2C_3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2: Continunity and Differentiation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AMQ7kNFq7G4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bKpZsQVqFUj",
        "outputId": "29b13de4-7bc7-4458-b90e-a5141e20d36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "E\n",
            "AccumBounds(-1, 1)\n"
          ]
        }
      ],
      "source": [
        "# import library\n",
        "import sympy as sym\n",
        "\n",
        "\n",
        "# pythonic math expressions: add spaces, use single quotes and lowercases\n",
        "# declaring variable\n",
        "x = sym.symbols('x')\n",
        "\n",
        "#limit of f as x approaches 2 \n",
        "f = x**4\n",
        "print(sym.limit(f, x, 2))\n",
        "\n",
        "#limit of h as x approaches infinity \n",
        "h = ((x + 3) / (x + 2)) ** x\n",
        "print(sym.limit(h, x, sym.oo)) #resulta in e\n",
        "\n",
        "#limit of g as x approaches infinity \n",
        "g = sym.sin(x)\n",
        "print(sym.limit(g, x, sym.oo))  #oscillates between -1 and 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# declaring variables\n",
        "x = sym.symbols('x')\n",
        "\n",
        "# we take the derivative using diff\n",
        "# diff (function, independent variable, number of derivatives)\n",
        "\n",
        "exp1 = sym.diff(sym.sin(2*x), x) \n",
        "print('1st derivative: ', exp1) \n",
        "\n",
        "exp2 = sym.diff(sym.sin(2*x), x, 2) \n",
        "print('2nd derivative: ', exp2) \n",
        "\n",
        "exp3 = sym.diff(sym.sin(2*x), x, 3)\n",
        "print('3rd derivative: ', exp3) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_lv-v53qWoG",
        "outputId": "71c70a8f-fb4b-4ffe-a6cd-08c2cc635315"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st derivative:  2*cos(2*x)\n",
            "2nd derivative:  -4*sin(2*x)\n",
            "3rd derivative:  -8*cos(2*x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, z = sym.symbols('x, y, z')\n",
        "f = sym.exp(x * y * z)\n",
        "sym.diff(f, x)  #first partial derivative with respects to x, f_(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "nVXOIQkhqmJ9",
        "outputId": "b379c9b8-5932-4aa8-d286-a0a5bd5700a0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "y*z*exp(x*y*z)"
            ],
            "text/latex": "$\\displaystyle y z e^{x y z}$"
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sym.diff(f, x, y) #second partial derivative with respects to y, f_(xy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "S-UmFe5iEKOI",
        "outputId": "39e6a07a-d840-4954-b499-3f9d4672b6fe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "z*(x*y*z + 1)*exp(x*y*z)"
            ],
            "text/latex": "$\\displaystyle z \\left(x y z + 1\\right) e^{x y z}$"
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sym.diff(f, x, y, z) #third partial derivative with respects to z, f_(xyz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "AXWZtqpyEMqw",
        "outputId": "2aa7297e-e915-4507-9950-98e8a268a2c9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(x**2*y**2*z**2 + 3*x*y*z + 1)*exp(x*y*z)"
            ],
            "text/latex": "$\\displaystyle \\left(x^{2} y^{2} z^{2} + 3 x y z + 1\\right) e^{x y z}$"
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3.3: Unconstrained Optimization**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H2z_bWyc6tzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In unconstrained optimization, the objective function may be a single-variable or multi-variable function, and the goal is to find the value(s) of the variable(s) that minimize or maximize the objective function. This is achieved by iteratively updating the values of the variable(s) in the direction that leads to a decrease (or increase) in the value of the objective function. The algorithm used to solve the optimization problem depends on the specific problem, but many methods such as gradient descent, Newton's method, and conjugate gradient method can be used."
      ],
      "metadata": {
        "id": "vXra-xbq6ssT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples: Optimize the Rosenbrock function to find its minimums. "
      ],
      "metadata": {
        "id": "3NveflKE7gPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nelder-Mead Simplex algorithm\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def rosen(x):\n",
        "    \"\"\"The Rosenbrock function\"\"\"\n",
        "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
        "\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "res = minimize(rosen, x0, method='nelder-mead',\n",
        "               options={'xatol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48O-DzTgyeSo",
        "outputId": "269b1ebb-0727-4054-94d0-000d02cee61c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 339\n",
            "         Function evaluations: 571\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplex algorithm is perhaps the most straightforward method for minimizing a well-behaved function. It solely relies on function evaluations and is suitable for uncomplicated minimization tasks. Nevertheless, since it doesn't employ any gradient evaluations, it may take more time to locate the minimum."
      ],
      "metadata": {
        "id": "7STKCCkp51jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Broyden-Fletcher-Goldfarb-Shanno algorithm\n",
        "def rosen_der(x):\n",
        "    xm = x[1:-1]\n",
        "    xm_m1 = x[:-2]\n",
        "    xm_p1 = x[2:]\n",
        "    der = np.zeros_like(x)\n",
        "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
        "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
        "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
        "    return der\n",
        "\n",
        "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
        "               options={'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkxoLDQT3Pal",
        "outputId": "5ac198d4-6a53-4622-d8c1-44a76453f7c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 25\n",
            "         Function evaluations: 30\n",
            "         Gradient evaluations: 30\n",
            "[1.00000004 1.0000001  1.00000021 1.00000044 1.00000092]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To expedite convergence towards the solution, this procedure utilizes the objective function's gradient. In the event that the user doesn't provide the gradient, it's computed using first-differences. When the gradient needs to be estimated, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) technique typically necessitates fewer function calls than the simplex algorithm."
      ],
      "metadata": {
        "id": "EoQ7D5si6MdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Newton-Conjugate-Gradient algorithm\n",
        "def rosen_hess(x):\n",
        "    x = np.asarray(x)\n",
        "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
        "    diagonal = np.zeros_like(x)\n",
        "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
        "    diagonal[-1] = 200\n",
        "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
        "    H = H + np.diag(diagonal)\n",
        "    return H\n",
        "\n",
        "res = minimize(rosen, x0, method='Newton-CG',\n",
        "               jac=rosen_der, hess=rosen_hess,\n",
        "               options={'xtol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELG2dB-zyh8h",
        "outputId": "ed3967d6-ee0d-47b4-8eb1-69e1e3024428"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.000000\n",
            "         Iterations: 24\n",
            "         Function evaluations: 33\n",
            "         Gradient evaluations: 33\n",
            "         Hessian evaluations: 24\n",
            "[1.         1.         1.         0.99999999 0.99999999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Newton-Conjugate Gradient algorithm modifies Newton's method and leverages a conjugate gradient algorithm to approximate the local Hessian matrix [NW]. Newton's method involves locally approximating the function with a quadratic form."
      ],
      "metadata": {
        "id": "4OJb0h2z8Uwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sorce:\n",
        "https://docs.scipy.org/doc/scipy/tutorial/optimize.html#"
      ],
      "metadata": {
        "id": "gInO98TUtxAX"
      }
    }
  ]
}